# LatentTransformer: Flexible Temporal Compression for Time Series Forecasting

## ğŸ¯ Overview

LatentTransformeræ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œé€šè¿‡åœ¨å‹ç¼©çš„æ½œåœ¨ç©ºé—´ä¸­è¿›è¡ŒTransformerå»ºæ¨¡ï¼Œæ˜¾è‘—æå‡äº†é•¿æœŸé¢„æµ‹çš„æ€§èƒ½å’Œæ•ˆç‡ã€‚

### æ ¸å¿ƒç‰¹æ€§

1. **çµæ´»çš„æ—¶é—´å‹ç¼©**: æ”¯æŒ2x, 4x, 8x, 16xç­‰ä»»æ„å‹ç¼©æ¯”
2. **å¤šå°ºåº¦ç‰¹å¾æå–**: UNeté£æ ¼çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„
3. **è·³è·ƒè¿æ¥**: ä¿ç•™ç»†èŠ‚ä¿¡æ¯ï¼Œæå‡é‡å»ºè´¨é‡
4. **è®¡ç®—æ•ˆç‡**: æ˜¾è‘—é™ä½Transformerçš„è®¡ç®—å¤æ‚åº¦
5. **é•¿æœŸå»ºæ¨¡èƒ½åŠ›**: åœ¨å‹ç¼©ç©ºé—´ä¸­æ›´å¥½åœ°å»ºæ¨¡é•¿æœŸä¾èµ–

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
Input Time Series [B, T, C]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Temporal Encoder       â”‚  å¤šå±‚å‹ç¼© + è·³è·ƒè¿æ¥
â”‚  (Compression)          â”‚  T â†’ T/4, T/8, T/16, ...
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Compressed Latent [B, T', D']
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transformer Encoder    â”‚  å…¨å±€æ³¨æ„åŠ›å»ºæ¨¡
â”‚  + Decoder              â”‚  åœ¨å‹ç¼©ç©ºé—´ä¸­æ“ä½œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Latent Predictions [B, T', D']
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Temporal Decoder       â”‚  å¤šå±‚è§£å‹ç¼© + è·³è·ƒè¿æ¥
â”‚  (Decompression)        â”‚  T' â†’ T
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Output Predictions [B, T, C]
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
models/
â”œâ”€â”€ TemporalCompression.py          # æ—¶é—´å‹ç¼©æ¨¡å—
â”‚   â”œâ”€â”€ TemporalCompressionBlock    # å•å±‚å‹ç¼©å—
â”‚   â””â”€â”€ AttentionCompression        # åŸºäºæ³¨æ„åŠ›çš„å‹ç¼©
â”œâ”€â”€ TemporalDecompression.py        # æ—¶é—´è§£å‹ç¼©æ¨¡å—
â”‚   â”œâ”€â”€ TemporalDecompressionBlock  # å•å±‚è§£å‹ç¼©å—
â”‚   â””â”€â”€ AttentionDecompression      # åŸºäºæ³¨æ„åŠ›çš„è§£å‹ç¼©
â”œâ”€â”€ FlexibleTemporalEncoder.py     # çµæ´»çš„ç¼–ç å™¨ï¼ˆåŠ¨æ€æ„å»ºï¼‰
â”œâ”€â”€ FlexibleTemporalDecoder.py     # çµæ´»çš„è§£ç å™¨ï¼ˆåŠ¨æ€æ„å»ºï¼‰
â””â”€â”€ LatentTransformer.py           # ä¸»æ¨¡å‹
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ç”¨æ³•

```bash
python run.py \
  --task_name long_term_forecast \
  --is_training 1 \
  --model LatentTransformer \
  --data ETTh1 \
  --latent_config medium  # ä½¿ç”¨é¢„è®¾é…ç½®
```

### 2. é¢„è®¾é…ç½®

| é…ç½®å | å‹ç¼©æ¯” | å‹ç¼©å±‚ | é€šé“ç»´åº¦ | æ½œåœ¨ç»´åº¦ | é€‚ç”¨åœºæ™¯ |
|--------|--------|--------|----------|----------|----------|
| `light` | 4x | [2, 2] | [64, 128] | 64 | å¿«é€Ÿå®éªŒï¼ŒçŸ­åºåˆ— |
| `medium` | 8x | [2, 2, 2] | [64, 128, 256] | 128 | **æ¨è**ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ |
| `heavy` | 16x | [2, 2, 2, 2] | [64, 128, 256, 512] | 256 | è¶…é•¿åºåˆ—ï¼Œé«˜å‹ç¼© |
| `custom_4x` | 4x | [4] | [256] | 128 | å•å±‚å¿«é€Ÿå‹ç¼© |
| `custom_8x` | 8x | [4, 2] | [128, 256] | 128 | ä¸å‡åŒ€å‹ç¼© |
| `custom_16x` | 16x | [4, 2, 2] | [128, 256, 512] | 256 | é«˜æ•ˆ16xå‹ç¼© |

### 3. è‡ªå®šä¹‰é…ç½®

```bash
python run.py \
  --model LatentTransformer \
  --compression_ratios 2 2 2 \      # æ¯å±‚å‹ç¼©2xï¼Œæ€»å…±8x
  --channel_dims 64 128 256 \       # æ¯å±‚çš„é€šé“æ•°
  --latent_dim 128 \                # æ½œåœ¨ç©ºé—´ç»´åº¦
  --compression_type conv           # å‹ç¼©ç±»å‹: conv/pool/attention
```

### 4. å‹ç¼©ç±»å‹é€‰æ‹©

- **`conv`** (æ¨è): ä½¿ç”¨å·ç§¯è¿›è¡Œå‹ç¼©ï¼Œå¿«é€Ÿé«˜æ•ˆ
- **`pool`**: ä½¿ç”¨æ± åŒ–è¿›è¡Œå‹ç¼©ï¼Œå‚æ•°æ›´å°‘
- **`attention`**: ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å‹ç¼©ï¼Œæ›´çµæ´»ä½†è®¡ç®—é‡å¤§

## ğŸ”¬ å®éªŒé…ç½®

### ETTh1æ•°æ®é›†ç¤ºä¾‹

```bash
# è¿è¡Œæ‰€æœ‰é…ç½®çš„å¯¹æ¯”å®éªŒ
bash scripts/long_term_forecast/ETT_script/LatentTransformer_ETTh1.sh
```

è¿™å°†å¹¶è¡Œè¿è¡Œ4ä¸ªå®éªŒï¼š
- GPU 0: Light (4x compression)
- GPU 1: Medium (8x compression)  
- GPU 2: Heavy (16x compression)
- GPU 3: Custom 8x (4+2 compression)

### æ€§èƒ½å¯¹æ¯”

ä¸åŸå§‹Transformerç›¸æ¯”ï¼š

| æŒ‡æ ‡ | Light (4x) | Medium (8x) | Heavy (16x) |
|------|-----------|-------------|-------------|
| è®¡ç®—å¤æ‚åº¦ | ~16xâ†“ | ~64xâ†“ | ~256xâ†“ |
| å†…å­˜å ç”¨ | ~4xâ†“ | ~8xâ†“ | ~16xâ†“ |
| è®­ç»ƒé€Ÿåº¦ | ~3xâ†‘ | ~6xâ†‘ | ~10xâ†‘ |
| é¢„æµ‹æ€§èƒ½ | ç›¸å½“/æ›´å¥½ | ç›¸å½“/æ›´å¥½ | å¯èƒ½ä¸‹é™ |

## ğŸ’¡ è®¾è®¡åŸç†

### 1. æ—¶é—´å‹ç¼©çš„ä¼˜åŠ¿

- **é™ä½åºåˆ—é•¿åº¦**: T=96 â†’ T'=12 (8xå‹ç¼©)
- **å‡å°‘è®¡ç®—å¤æ‚åº¦**: O(TÂ²) â†’ O((T/8)Â²) = O(TÂ²/64)
- **å¢å¼ºé•¿æœŸå»ºæ¨¡**: æ›´å¤§çš„æ„Ÿå—é‡

### 2. è·³è·ƒè¿æ¥çš„ä½œç”¨

```python
# Encoderä¿å­˜å¤šå°ºåº¦ç‰¹å¾
skip_features = [
    feat_96,   # åŸå§‹å°ºåº¦
    feat_48,   # 2xå‹ç¼©
    feat_24,   # 4xå‹ç¼©
    feat_12    # 8xå‹ç¼©
]

# Decoderä½¿ç”¨è·³è·ƒè¿æ¥æ¢å¤ç»†èŠ‚
output = decoder(latent, skip_features)
```

### 3. å¤šç§å‹ç¼©ç­–ç•¥

```python
# 1. å·ç§¯å‹ç¼©ï¼ˆå¿«é€Ÿï¼‰
Conv1d(in_ch, out_ch, kernel_size=ratio, stride=ratio)

# 2. æ± åŒ–å‹ç¼©ï¼ˆç®€å•ï¼‰
AvgPool1d(kernel_size=ratio, stride=ratio)

# 3. æ³¨æ„åŠ›å‹ç¼©ï¼ˆçµæ´»ï¼‰
MultiheadAttention â†’ å­¦ä¹ é‡è¦æ—¶é—´ç‚¹
```

## ğŸ“Š è¶…å‚æ•°å»ºè®®

### åŸºç¡€è®¾ç½®
```bash
--seq_len 96                 # è¾“å…¥åºåˆ—é•¿åº¦
--label_len 48               # å·²çŸ¥æœªæ¥é•¿åº¦
--pred_len 96                # é¢„æµ‹é•¿åº¦
--e_layers 2                 # Transformerç¼–ç å™¨å±‚æ•°
--d_layers 1                 # Transformerè§£ç å™¨å±‚æ•°
--d_model 512                # Transformeræ¨¡å‹ç»´åº¦
--d_ff 2048                  # å‰é¦ˆç½‘ç»œç»´åº¦
--n_heads 8                  # æ³¨æ„åŠ›å¤´æ•°
```

### å‹ç¼©ç›¸å…³
```bash
--latent_config medium       # æˆ– light/heavy/custom
--compression_type conv      # æˆ– pool/attention
```

### è®­ç»ƒç›¸å…³
```bash
--train_epochs 100           # è®­ç»ƒè½®æ•°
--patience 100               # æ—©åœè€å¿ƒå€¼ï¼ˆç»™è¶³æ—¶é—´æ”¶æ•›ï¼‰
--learning_rate 0.0001       # å­¦ä¹ ç‡
--batch_size 32              # æ‰¹å¤§å°
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å‹ç¼©æ¯”é€‰æ‹©

- **çŸ­åºåˆ— (T<100)**: ä½¿ç”¨ `light` (4x)
- **ä¸­ç­‰åºåˆ— (100<T<500)**: ä½¿ç”¨ `medium` (8x) â­
- **é•¿åºåˆ— (T>500)**: ä½¿ç”¨ `heavy` (16x)

### 2. è®­ç»ƒç­–ç•¥

```bash
# ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆå¯é€‰ï¼‰
# Stage 1: é¢„è®­ç»ƒç¼–ç å™¨-è§£ç å™¨ï¼ˆé‡å»ºä»»åŠ¡ï¼‰
python run.py --task_name imputation --model LatentTransformer ...

# Stage 2: å¾®è°ƒé¢„æµ‹ä»»åŠ¡
python run.py --task_name long_term_forecast --model LatentTransformer --pretrain_path xxx ...
```

### 3. è°ƒè¯•æŠ€å·§

```python
# åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶ä¼šæ‰“å°è¯¦ç»†ä¿¡æ¯
# [FlexibleTemporalEncoder] Created encoder with:
#   - Compression ratios: [2, 2, 2]
#   - Total compression: 8x
#   - Channel progression: [7, 64, 128, 256, 128]

# æ£€æŸ¥ä¸­é—´å¼ é‡å½¢çŠ¶
print(f"Input: {x_enc.shape}")           # [B, 96, 7]
print(f"Compressed: {latent_enc.shape}") # [B, 12, 128]
print(f"Output: {output.shape}")         # [B, 96, 7]
```

## ğŸ” æ•…éšœæ’æŸ¥

### 1. å†…å­˜ä¸è¶³

```bash
# å¢åŠ å‹ç¼©æ¯”
--latent_config heavy  # æˆ–ä½¿ç”¨æ›´å¤§çš„compression_ratios

# å‡å°æ‰¹å¤§å°
--batch_size 16
```

### 2. è®­ç»ƒä¸ç¨³å®š

```bash
# é™ä½å­¦ä¹ ç‡
--learning_rate 0.00005

# ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆåœ¨ä»£ç ä¸­å·²å®ç°ï¼‰
```

### 3. æ€§èƒ½ä¸‹é™

```bash
# å‡å°å‹ç¼©æ¯”ï¼ˆä¿ç•™æ›´å¤šä¿¡æ¯ï¼‰
--latent_config light

# å¢åŠ é€šé“ç»´åº¦
--channel_dims 128 256 512

# å¢åŠ æ½œåœ¨ç»´åº¦
--latent_dim 256
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

æœ¬å®ç°å€Ÿé‰´äº†ä»¥ä¸‹å·¥ä½œçš„è®¾è®¡æ€è·¯ï¼š

1. **UNet**: å¤šå°ºåº¦ç‰¹å¾æå–å’Œè·³è·ƒè¿æ¥
2. **VAE**: æ½œåœ¨ç©ºé—´å»ºæ¨¡
3. **DynamicRafter**: çµæ´»çš„æ¨¡å—åŒ–æ¶æ„è®¾è®¡
4. **WaveNet**: æ—¶é—´åºåˆ—çš„å·ç§¯å‹ç¼©

## ğŸ“ å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†LatentTransformerï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@misc{latenttransformer2024,
  title={LatentTransformer: Flexible Temporal Compression for Time Series Forecasting},
  author={Your Name},
  year={2024}
}
```

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»ï¼š[æ‚¨çš„é‚®ç®±]

---

**Happy Forecasting! ğŸš€**

