# LatentTransformeré‡å»ºæ¨¡å¼å®Œæ•´æŒ‡å—

## ğŸ“‹ æ¦‚è§ˆ

ç°åœ¨æˆ‘ä»¬çš„**LatentTransformer**æ”¯æŒ**ä¸‰ç§é‡å»ºæ¨¡å¼**ï¼Œæ¯ç§æ¨¡å¼éƒ½æœ‰ä¸åŒçš„ç›®æ ‡å’Œä¼˜åŠ¿ï¼š

### å½“å‰å®ç°çš„Decoderç±»å‹

| æ¨¡å¼ | ç±»å‹ | é‡å»ºæŸå¤± | KLæ•£åº¦ | Masking | ç”¨é€” |
|------|------|----------|--------|---------|------|
| **åŸå§‹LatentTransformer** | æ— é‡å»º | âŒ | âŒ | âŒ | çº¯é¢„æµ‹ä»»åŠ¡ |
| **AE** | Auto-Encoder | âœ… | âŒ | âŒ | å­¦ä¹ æ›´å¥½çš„è¡¨ç¤º |
| **VAE** | Variational AE | âœ… | âœ… | âŒ | æ¦‚ç‡å»ºæ¨¡+æ³›åŒ– |
| **MAE** | Masked AE | âœ… | âŒ | âœ… | é²æ£’è¡¨ç¤ºå­¦ä¹  |

## ğŸ—ï¸ æ¶æ„å¯¹æ¯”

### 1. åŸå§‹LatentTransformer (æ— é‡å»º)
```
Input â†’ Encoder â†’ Latent â†’ Transformer â†’ Latent' â†’ Decoder â†’ Prediction
                                                              â†“
                                                         åªæœ‰é¢„æµ‹æŸå¤±
```

### 2. AEæ¨¡å¼ (Auto-Encoder)
```
Input â†’ Encoder â†’ Latent â†’ Transformer â†’ Latent' â†’ Decoder â†’ Prediction
   â†“              â†“                                              â†“
   â””â”€â”€â”€â”€â”€â”€â†’ Reconstruction Decoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Reconstructed Input
                                                              â†“
                                               é¢„æµ‹æŸå¤± + é‡å»ºæŸå¤±
```

### 3. VAEæ¨¡å¼ (Variational Auto-Encoder)
```
Input â†’ Encoder â†’ [Î¼, ÏƒÂ²] â†’ Reparameterization â†’ Latent â†’ Transformer â†’ Prediction
   â†“                â†“                                â†“
   â”‚         KL Divergence Loss                     â”‚
   â”‚                                                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Reconstruction Decoder â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
           é¢„æµ‹æŸå¤± + é‡å»ºæŸå¤± + KLæ•£åº¦æŸå¤±
```

### 4. MAEæ¨¡å¼ (Masked Auto-Encoder)
```
Input â†’ Random Masking â†’ Masked Input â†’ Encoder â†’ Latent â†’ Transformer â†’ Prediction
   â†“                                       â†“
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Reconstruction Decoder â†â”˜
                        â†“
                Only Maskedéƒ¨åˆ†é‡å»º
                        â†“
           é¢„æµ‹æŸå¤± + Maskedé‡å»ºæŸå¤±
```

## ğŸ¯ æŸå¤±å‡½æ•°è¯¦è§£

### 1. åŸå§‹LatentTransformer
```python
total_loss = prediction_loss
```

### 2. AEæ¨¡å¼
```python
total_loss = prediction_loss + Î»_recon * reconstruction_loss

where:
- reconstruction_loss = MSE(reconstructed_input, original_input)
- Î»_recon = reconstruction_loss_weight (default: 0.5)
```

### 3. VAEæ¨¡å¼
```python
total_loss = prediction_loss + Î»_recon * reconstruction_loss + Î»_kl * kl_loss

where:
- reconstruction_loss = MSE(reconstructed_input, original_input)
- kl_loss = -0.5 * Î£(1 + log(ÏƒÂ²) - Î¼Â² - ÏƒÂ²)
- Î»_recon = reconstruction_loss_weight (default: 0.5)
- Î»_kl = kl_loss_weight (default: 0.01)
```

### 4. MAEæ¨¡å¼
```python
total_loss = prediction_loss + Î»_recon * masked_reconstruction_loss

where:
- masked_reconstruction_loss = MSE(reconstructed[masked], original[masked])
- åªè®¡ç®—è¢«maskéƒ¨åˆ†çš„é‡å»ºæŸå¤±
- Î»_recon = reconstruction_loss_weight (default: 0.5)
```

## ğŸ’» ä½¿ç”¨æ–¹æ³•

### 1. æ— é‡å»ºæ¨¡å¼ï¼ˆåŸå§‹LatentTransformerï¼‰
```bash
python run.py \
  --model LatentTransformer \
  --task_name long_term_forecast \
  --data ETTh1 \
  --latent_config medium \
  # å…¶ä»–å‚æ•°...
```

### 2. AEæ¨¡å¼
```bash
python run.py \
  --model LatentTransformerWithReconstruction \
  --reconstruction_mode AE \
  --reconstruction_loss_weight 0.5 \
  --task_name long_term_forecast \
  --data ETTh1 \
  --latent_config medium \
  # å…¶ä»–å‚æ•°...
```

### 3. VAEæ¨¡å¼
```bash
python run.py \
  --model LatentTransformerWithReconstruction \
  --reconstruction_mode VAE \
  --reconstruction_loss_weight 0.5 \
  --kl_loss_weight 0.01 \
  --task_name long_term_forecast \
  --data ETTh1 \
  --latent_config medium \
  # å…¶ä»–å‚æ•°...
```

### 4. MAEæ¨¡å¼
```bash
python run.py \
  --model LatentTransformerWithReconstruction \
  --reconstruction_mode MAE \
  --reconstruction_loss_weight 0.5 \
  --mask_ratio 0.25 \
  --task_name long_term_forecast \
  --data ETTh1 \
  --latent_config medium \
  # å…¶ä»–å‚æ•°...
```

### 5. è¿è¡Œå¯¹æ¯”å®éªŒ
```bash
bash scripts/long_term_forecast/ETT_script/LatentTransformer_Reconstruction_Comparison.sh
```

## ğŸ“Š è¶…å‚æ•°å»ºè®®

### é‡å»ºæŸå¤±æƒé‡ (`reconstruction_loss_weight`)
- **æ¨èèŒƒå›´**: 0.1 ~ 1.0
- **é»˜è®¤å€¼**: 0.5
- **è¯´æ˜**: 
  - å¤ªå°ï¼šé‡å»ºæŸå¤±ä½œç”¨ä¸æ˜æ˜¾
  - å¤ªå¤§ï¼šå¯èƒ½å½±å“é¢„æµ‹æ€§èƒ½
  - å»ºè®®ä»0.5å¼€å§‹è°ƒæ•´

### KLæ•£åº¦æƒé‡ (`kl_loss_weight`, VAEä¸“ç”¨)
- **æ¨èèŒƒå›´**: 0.001 ~ 0.1
- **é»˜è®¤å€¼**: 0.01
- **è¯´æ˜**:
  - å¤ªå°ï¼šæ½œåœ¨ç©ºé—´ä¸å¤Ÿæ­£åˆ™åŒ–
  - å¤ªå¤§ï¼šé‡å»ºè´¨é‡ä¸‹é™ï¼ˆposterior collapseï¼‰
  - é€šå¸¸è®¾ç½®ä¸ºreconstruction_loss_weightçš„1/10~1/100

### Maskingæ¯”ä¾‹ (`mask_ratio`, MAEä¸“ç”¨)
- **æ¨èèŒƒå›´**: 0.15 ~ 0.75
- **é»˜è®¤å€¼**: 0.25
- **è¯´æ˜**:
  - 0.25: é€‚åˆæ—¶é—´åºåˆ—ï¼ˆç±»æ¯”BERTçš„15%ï¼‰
  - 0.50: æ›´å¼ºçš„è‡ªç›‘ç£ä¿¡å·
  - 0.75: MAEè®ºæ–‡ä¸­å›¾åƒçš„è®¾ç½®ï¼ˆå¯èƒ½å¯¹æ—¶é—´åºåˆ—å¤ªaggressiveï¼‰

## ğŸ” å„æ¨¡å¼çš„ä¼˜ç¼ºç‚¹

### åŸå§‹LatentTransformer (æ— é‡å»º)
âœ… **ä¼˜ç‚¹:**
- æœ€ç®€å•ï¼Œå‚æ•°æœ€å°‘
- è®­ç»ƒé€Ÿåº¦æœ€å¿«
- ä¸“æ³¨äºé¢„æµ‹ä»»åŠ¡

âš ï¸ **ç¼ºç‚¹:**
- å¯èƒ½å­¦ä¸åˆ°æœ€ä¼˜çš„æ½œåœ¨è¡¨ç¤º
- ç¼–ç å™¨æ²¡æœ‰æ˜¾å¼çš„è¡¨ç¤ºå­¦ä¹ ç›®æ ‡

### AEæ¨¡å¼
âœ… **ä¼˜ç‚¹:**
- æ˜¾å¼çš„è¡¨ç¤ºå­¦ä¹ ç›®æ ‡
- é‡å»ºæŸå¤±å¸®åŠ©å­¦ä¹ æ›´å¥½çš„ç‰¹å¾
- ç¡®å®šæ€§ç¼–ç ï¼Œå®¹æ˜“ç†è§£

âš ï¸ **ç¼ºç‚¹:**
- å¯èƒ½è¿‡æ‹Ÿåˆåˆ°è®­ç»ƒæ•°æ®
- æ½œåœ¨ç©ºé—´å¯èƒ½ä¸å¤Ÿsmooth

### VAEæ¨¡å¼
âœ… **ä¼˜ç‚¹:**
- æ¦‚ç‡å»ºæ¨¡ï¼Œæä¾›ä¸ç¡®å®šæ€§ä¼°è®¡
- KLæ•£åº¦æ­£åˆ™åŒ–æ½œåœ¨ç©ºé—´
- æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
- æ½œåœ¨ç©ºé—´æ›´è¿ç»­smooth

âš ï¸ **ç¼ºç‚¹:**
- éœ€è¦è°ƒæ•´KLæƒé‡ï¼ˆbalance problemï¼‰
- å¯èƒ½å‡ºç°posterior collapse
- è®­ç»ƒç¨å¾®å¤æ‚

### MAEæ¨¡å¼
âœ… **ä¼˜ç‚¹:**
- ç±»ä¼¼BERTçš„é¢„è®­ç»ƒèŒƒå¼
- å­¦ä¹ æ›´é²æ£’çš„è¡¨ç¤º
- å¯¹å™ªå£°å’Œç¼ºå¤±å€¼æ›´é²æ£’
- å¯ä»¥ä½œä¸ºé¢„è®­ç»ƒæ–¹æ³•

âš ï¸ **ç¼ºç‚¹:**
- éœ€è¦è°ƒæ•´maskingæ¯”ä¾‹
- è®­ç»ƒæ—¶é—´ç¨é•¿ï¼ˆmaskingæ“ä½œï¼‰
- å¯¹æ—¶é—´åºåˆ—çš„æœ€ä¼˜maskingç­–ç•¥éœ€è¦æ¢ç´¢

## ğŸ“ˆ å®éªŒå»ºè®®

### 1. åŸºç¡€å¯¹æ¯”å®éªŒ
```bash
# è¿è¡Œ4ä¸ªæ¨¡å¼çš„å¯¹æ¯”
bash scripts/long_term_forecast/ETT_script/LatentTransformer_Reconstruction_Comparison.sh
```

è¿™å°†è¿è¡Œï¼š
- GPU 0: æ— é‡å»ºï¼ˆbaselineï¼‰
- GPU 1: AEæ¨¡å¼
- GPU 2: VAEæ¨¡å¼
- GPU 3: MAEæ¨¡å¼

### 2. è¶…å‚æ•°æœç´¢

#### AEè¶…å‚æ•°æœç´¢
```bash
for weight in 0.1 0.3 0.5 0.7 1.0; do
    python run.py \
      --model LatentTransformerWithReconstruction \
      --reconstruction_mode AE \
      --reconstruction_loss_weight $weight \
      --model_id ETTh1_AE_w${weight} \
      # å…¶ä»–å‚æ•°...
done
```

#### VAEè¶…å‚æ•°æœç´¢
```bash
for kl_weight in 0.001 0.01 0.1; do
    python run.py \
      --model LatentTransformerWithReconstruction \
      --reconstruction_mode VAE \
      --reconstruction_loss_weight 0.5 \
      --kl_loss_weight $kl_weight \
      --model_id ETTh1_VAE_kl${kl_weight} \
      # å…¶ä»–å‚æ•°...
done
```

#### MAEè¶…å‚æ•°æœç´¢
```bash
for mask_ratio in 0.15 0.25 0.5 0.75; do
    python run.py \
      --model LatentTransformerWithReconstruction \
      --reconstruction_mode MAE \
      --mask_ratio $mask_ratio \
      --model_id ETTh1_MAE_mask${mask_ratio} \
      # å…¶ä»–å‚æ•°...
done
```

### 3. è¯„ä¼°æŒ‡æ ‡

éœ€è¦å…³æ³¨çš„æŒ‡æ ‡ï¼š
1. **é¢„æµ‹æ€§èƒ½**: MSE, MAE on test set
2. **é‡å»ºè´¨é‡**: Reconstruction MSE (è®­ç»ƒæ—¶)
3. **KLæ•£åº¦**: KL loss value (VAE)
4. **è®­ç»ƒæ•ˆç‡**: Training time per epoch
5. **æ”¶æ•›é€Ÿåº¦**: Epochs to converge

## ğŸ”§ ä¸exp_long_term_forecasting.pyçš„é›†æˆ

### è®­ç»ƒLossè®¡ç®—

å½“å‰çš„`exp_long_term_forecasting.py`å·²ç»æ”¯æŒreconstruction lossï¼š

```python
# åœ¨train()æ–¹æ³•ä¸­
if self.args.reconstruction_mode != 'None':
    outputs, reconstructed_input, reconstruction_loss, kl_loss = self.model(...)
    
    # è®¡ç®—æ€»æŸå¤±
    loss = pred_loss + \
           self.args.reconstruction_loss_weight * reconstruction_loss + \
           self.args.kl_loss_weight * kl_loss
else:
    outputs = self.model(...)
    loss = pred_loss
```

### éªŒè¯å’Œæµ‹è¯•

éªŒè¯å’Œæµ‹è¯•æ—¶**ä¸ä½¿ç”¨**é‡å»ºæŸå¤±ï¼Œåªè¯„ä¼°é¢„æµ‹æ€§èƒ½ï¼š

```python
# åœ¨vali()æ–¹æ³•ä¸­
if self.args.reconstruction_mode != 'None':
    outputs, _, _, _ = self.model(...)  # å¿½ç•¥é‡å»ºè¾“å‡º
else:
    outputs = self.model(...)

# åªè®¡ç®—é¢„æµ‹æŸå¤±
loss = criterion(pred_outputs, pred_targets)
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. è®­ç»ƒç­–ç•¥

#### æ–¹æ³•1: ç«¯åˆ°ç«¯è®­ç»ƒ
```bash
python run.py \
  --model LatentTransformerWithReconstruction \
  --reconstruction_mode VAE \
  --train_epochs 100
```

#### æ–¹æ³•2: ä¸¤é˜¶æ®µè®­ç»ƒï¼ˆæ¨èï¼‰
```bash
# Stage 1: é¢„è®­ç»ƒencoder-decoder (é‡å»ºä»»åŠ¡)
python run.py \
  --model LatentTransformerWithReconstruction \
  --reconstruction_mode MAE \
  --reconstruction_loss_weight 1.0 \  # æ›´é«˜çš„é‡å»ºæƒé‡
  --train_epochs 50 \
  --model_id ETTh1_pretrain

# Stage 2: å¾®è°ƒé¢„æµ‹ä»»åŠ¡
python run.py \
  --model LatentTransformerWithReconstruction \
  --reconstruction_mode MAE \
  --reconstruction_loss_weight 0.1 \  # é™ä½é‡å»ºæƒé‡
  --pretrain_path checkpoints/ETTh1_pretrain/checkpoint.pth \
  --train_epochs 50 \
  --model_id ETTh1_finetune
```

### 2. æ¨¡å¼é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èæ¨¡å¼ | åŸå›  |
|------|----------|------|
| æ•°æ®å……è¶³ï¼Œè¿½æ±‚æœ€ä¼˜æ€§èƒ½ | AE | ç®€å•æœ‰æ•ˆ |
| æ•°æ®æœ‰é™ï¼Œéœ€è¦æ³›åŒ– | VAE | æ­£åˆ™åŒ–æ•ˆæœå¥½ |
| æ•°æ®æœ‰å™ªå£°/ç¼ºå¤± | MAE | é²æ£’æ€§å¼º |
| éœ€è¦ä¸ç¡®å®šæ€§ä¼°è®¡ | VAE | æ¦‚ç‡å»ºæ¨¡ |
| è¿½æ±‚é€Ÿåº¦ | æ— é‡å»º | æœ€å¿« |
| é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ | MAE | ç±»ä¼¼BERT |

### 3. è°ƒè¯•æŠ€å·§

#### æ£€æŸ¥é‡å»ºè´¨é‡
```python
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ 
if reconstruction_mode != 'None':
    print(f"Reconstruction Loss: {reconstruction_loss.item():.4f}")
    print(f"Prediction Loss: {pred_loss.item():.4f}")
    
    # å¯è§†åŒ–é‡å»º
    import matplotlib.pyplot as plt
    plt.plot(original_input[0, :, 0].cpu(), label='Original')
    plt.plot(reconstructed_input[0, :, 0].cpu(), label='Reconstructed')
    plt.legend()
    plt.savefig('reconstruction.png')
```

#### æ£€æŸ¥VAEçš„KLæ•£åº¦
```python
# ç›‘æ§KL lossï¼Œé¿å…posterior collapse
if reconstruction_mode == 'VAE':
    print(f"KL Loss: {kl_loss.item():.6f}")
    # å¥åº·çš„KL lossåº”è¯¥åœ¨0.001~0.1ä¹‹é—´
    # å¦‚æœæ¥è¿‘0ï¼Œå¯èƒ½æ˜¯posterior collapse
```

#### æ£€æŸ¥MAEçš„maskingæ•ˆæœ
```python
# éªŒè¯maskingæ˜¯å¦æ­£ç¡®
if reconstruction_mode == 'MAE':
    print(f"Mask ratio: {mask.mean().item():.2f}")
    # åº”è¯¥æ¥è¿‘è®¾å®šçš„mask_ratio
```

## ğŸ“ ç†è®ºèƒŒæ™¯

### AE (Auto-Encoder)
- **ç›®æ ‡**: å­¦ä¹ æ•°æ®çš„ä½ç»´è¡¨ç¤º
- **æŸå¤±**: L = L_pred + Î» * L_recon
- **åº”ç”¨**: ç‰¹å¾å­¦ä¹ ï¼Œé™ç»´

### VAE (Variational Auto-Encoder)
- **ç›®æ ‡**: å­¦ä¹ æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒ
- **æŸå¤±**: L = L_pred + Î»_recon * L_recon + Î»_kl * KL(q(z|x) || p(z))
- **åº”ç”¨**: ç”Ÿæˆå»ºæ¨¡ï¼Œä¸ç¡®å®šæ€§ä¼°è®¡

### MAE (Masked Auto-Encoder)
- **ç›®æ ‡**: ä»éƒ¨åˆ†è§‚æµ‹é‡å»ºå®Œæ•´æ•°æ®
- **æŸå¤±**: L = L_pred + Î» * L_masked_recon
- **åº”ç”¨**: è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œé²æ£’è¡¨ç¤ºå­¦ä¹ 

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Auto-Encoder**: Hinton & Salakhutdinov, "Reducing the Dimensionality of Data with Neural Networks", Science 2006
2. **VAE**: Kingma & Welling, "Auto-Encoding Variational Bayes", ICLR 2014
3. **MAE**: He et al., "Masked Autoencoders Are Scalable Vision Learners", CVPR 2022
4. **BERT**: Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers", NAACL 2019

## âœ… æ€»ç»“

ç°åœ¨ä½ çš„LatentTransformeræ”¯æŒï¼š

| ç‰¹æ€§ | çŠ¶æ€ |
|------|------|
| âœ… æ— é‡å»ºæ¨¡å¼ | å·²å®ç° |
| âœ… AEæ¨¡å¼ | å·²å®ç° |
| âœ… VAEæ¨¡å¼ | å·²å®ç°ï¼ˆå¸¦KLæ•£åº¦ï¼‰ |
| âœ… MAEæ¨¡å¼ | å·²å®ç°ï¼ˆå¸¦maskingï¼‰ |
| âœ… è®­ç»ƒlossæ”¯æŒ | å·²é›†æˆåˆ°exp_long_term_forecasting.py |
| âœ… çµæ´»çš„è¶…å‚æ•° | reconstruction_loss_weight, kl_loss_weight, mask_ratio |
| âœ… å¯¹æ¯”å®éªŒè„šæœ¬ | LatentTransformer_Reconstruction_Comparison.sh |

ç¥å®éªŒé¡ºåˆ©ï¼ğŸš€

